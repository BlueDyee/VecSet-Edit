import os

from typing import Callable, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
from diffusers.models.attention_processor import Attention
from diffusers.models.embeddings import apply_rotary_emb

import numpy as np
import torch
import torch.nn as nn
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.models.attention_processor import Attention, AttentionProcessor
from diffusers.models.autoencoders.vae import DecoderOutput
from diffusers.models.modeling_outputs import AutoencoderKLOutput
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import FP32LayerNorm, LayerNorm
from diffusers.utils import logging
from diffusers.utils.accelerate_utils import apply_forward_hook
from einops import repeat
from torch_cluster import fps
from tqdm import tqdm

# # ä½¿ç”¨çµ•å°å°å…¥ï¼Œé¿å…ç›¸å°å°å…¥å•é¡Œ
# from triposg.models.attention_processor import FusedTripoSGAttnProcessor2_0, TripoSGAttnProcessor2_0, FlashTripoSGAttnProcessor2_0
# from triposg.models.embeddings import FrequencyPositionalEmbedding
# from triposg.models.transformers.triposg_transformer import DiTBlock
# from triposg.models.autoencoders.vae import DiagonalGaussianDistribution



class CustomAttnProcessor2_0:
    r"""
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the TripoSG model. It applies a s normalization layer and rotary embedding on query and key vector.
    """

    def __init__(self):
        if not hasattr(F, "scaled_dot_product_attention"):
            raise ImportError(
                "AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0."
            )

    def __call__(
        self,
        attn: Attention,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        temb: Optional[torch.Tensor] = None,
        image_rotary_emb: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:

        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)

        input_ndim = hidden_states.ndim

        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)

        batch_size, sequence_length, _ = (
            hidden_states.shape
            if encoder_hidden_states is None
            else encoder_hidden_states.shape
        )

        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(
                attention_mask, sequence_length, batch_size
            )
            # scaled_dot_product_attention expects attention_mask shape to be
            # (batch, heads, source_length, target_length)
            attention_mask = attention_mask.view(
                batch_size, attn.heads, -1, attention_mask.shape[-1]
            )

        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(
                1, 2
            )

        query = attn.to_q(hidden_states)

        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(
                encoder_hidden_states
            )

        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        # NOTE that pre-trained models split heads first then split qkv or kv, like .view(..., attn.heads, 3, dim)
        # instead of .view(..., 3, attn.heads, dim). So we need to re-split here.
        if not attn.is_cross_attention:
            qkv = torch.cat((query, key, value), dim=-1)
            split_size = qkv.shape[-1] // attn.heads // 3
            qkv = qkv.view(batch_size, -1, attn.heads, split_size * 3)
            query, key, value = torch.split(qkv, split_size, dim=-1)
        else:
            kv = torch.cat((key, value), dim=-1)
            split_size = kv.shape[-1] // attn.heads // 2
            kv = kv.view(batch_size, -1, attn.heads, split_size * 2)
            key, value = torch.split(kv, split_size, dim=-1)

        head_dim = key.shape[-1]

        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)

        # Apply RoPE if needed
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            if not attn.is_cross_attention:
                key = apply_rotary_emb(key, image_rotary_emb)

        # the output of sdp = (batch, num_heads, seq_len, head_dim)
        # TODO: add support for attn.scale when we move to Torch 2.1
        hidden_states = F.scaled_dot_product_attention(
            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False
        )

        # â€¼ï¸æ‰‹åŠ¨è®¡ç®— attention scores for Debugging
        #print("Query shape:", query.shape)
        #print("Key shape:", key.shape)
        query_dim = query.shape[-1]  
        scale = 1.0 / (query_dim ** 0.5)
        attention_scores = torch.matmul(query, key.transpose(-2, -1)) * scale
        #print("attention_scores shape:", attention_scores.shape)

        attention_probs = F.softmax(attention_scores, dim=-1)
        self.attn_probs=attention_probs
        self.attention  = F.scaled_dot_product_attention(
            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False
        )
        # â€¼ï¸
        
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, attn.heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        
        # linear proj
        hidden_states = attn.to_out[0](hidden_states)
        # dropout
        hidden_states = attn.to_out[1](hidden_states)

        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        
        if attn.residual_connection:
            hidden_states = hidden_states + residual

        hidden_states = hidden_states / attn.rescale_output_factor
        
        return hidden_states

class dec_cross_forward_override:
    def __init__(self):
        self.attn2_processor = CustomAttnProcessor2_0()
        self.vis_xyz_prob=[]
        self.vis_xyz_attn=[]
        self.vis_xyz_token_indices=[]
        self.call_count=0
        self.token_attention_distribution=[]
        # Should be initialized by vae control
        self.queries_mask=[]
        # self.rescale_xyz_metrics = torch.linspace(-1.0049, 1.0049, int(256), dtype=torch.float16)

    def register(self, block):
        # overwrite block.attn2.preocessor
        block.attn2.set_processor(self.attn2_processor)
        # ä½¿ç”¨é–‰åŒ…æ•ç² selfï¼ˆoverride å¯¦ä¾‹)
        def custom_forward(
            hidden_states: torch.Tensor,
            encoder_hidden_states: Optional[torch.Tensor] = None,
            encoder_hidden_states_2: Optional[torch.Tensor] = None,
            temb: Optional[torch.Tensor] = None,
            image_rotary_emb: Optional[torch.Tensor] = None,
            skip: Optional[torch.Tensor] = None,
            attention_kwargs= None,
            ) -> torch.Tensor:
            self.call_count+=1
            # Prepare attention kwargs
            attention_kwargs = attention_kwargs.copy() if attention_kwargs is not None else {}
            cross_attention_scale = attention_kwargs.pop("cross_attention_scale", 1.0)
            cross_attention_2_scale = attention_kwargs.pop("cross_attention_2_scale", 1.0)
            # 0. Long Skip Connection
            if block.skip_linear is not None:
                cat = torch.cat(
                    (
                        [skip, hidden_states]
                        if block.skip_concat_front
                        else [hidden_states, skip]
                    ),
                    dim=-1,
                )
                if block.skip_norm_last:
                    # don't do this
                    hidden_states = block.skip_linear(cat)
                    hidden_states = block.skip_norm(hidden_states)
                else:
                    cat = block.skip_norm(cat)
                    hidden_states = block.skip_linear(cat)

            # 1. Self-Attention
            if block.use_self_attention:
                norm_hidden_states = block.norm1(hidden_states)
                attn_output = block.attn1(
                    norm_hidden_states,
                    image_rotary_emb=image_rotary_emb,
                    **attention_kwargs,
                )
                hidden_states = hidden_states + attn_output

            # 2. Cross-Attention
            if block.use_cross_attention:
                if block.use_cross_attention_2:
                    hidden_states = (
                        hidden_states
                        + block.attn2(
                            block.norm2(hidden_states),
                            encoder_hidden_states=encoder_hidden_states,
                            image_rotary_emb=image_rotary_emb,
                            **attention_kwargs,
                        ) * cross_attention_scale
                        + block.attn2_2(
                            block.norm2_2(hidden_states),
                            encoder_hidden_states=encoder_hidden_states_2,
                            image_rotary_emb=image_rotary_emb,
                            **attention_kwargs,
                        ) * cross_attention_2_scale
                    )
                else:
                    # This attn2 is overide by custom_attention processor
                    attn_output= block.attn2(
                        block.norm2(hidden_states),
                        encoder_hidden_states=encoder_hidden_states,
                        image_rotary_emb=image_rotary_emb,
                        **attention_kwargs,
                    ) * cross_attention_scale
                    hidden_states = hidden_states + attn_output

                    
                    attn_probs_val=self.attn2_processor.attn_probs[0].mean(dim=0)
                    # Feature select grid points to perform sum
                    if self.queries_mask is not None:
                        sum_attn_distribution = attn_probs_val[self.queries_mask].sum(dim=0)
                    else:
                        sum_attn_distribution = attn_probs_val.sum(dim=0)
                    assert type(self.token_attention_distribution)==torch.Tensor, \
                        f"The list of token_attention_distribution should be initialize by vae control"
                    self.token_attention_distribution += sum_attn_distribution


                    # åªä¿ç•™ top5 çš„ token
                    top5_values, top5_indices = torch.topk(attn_probs_val, k=5, dim=-1)
                    #print(f"top5_indices.shape={top5_indices.shape} top5_values.shape={top5_values.shape}")
                    #print(f"attnprob.shape={attn_probs_val.shape}")
                    # ç›´æ¥ä½¿ç”¨topkè¿”å›çš„values
                    self.vis_xyz_token_indices.append(top5_indices.cpu())
                    self.vis_xyz_prob.append(top5_values.cpu())

            # FFN Layer ### TODO: switch norm2 and norm3 in the state dict
            mlp_inputs = block.norm3(hidden_states)
            hidden_states = hidden_states + block.ff(mlp_inputs)

            return hidden_states
            
        # ç›´æ¥è³¦å€¼ï¼Œä¸éœ€è¦ __get__
        block.forward = custom_forward

class DecoderOverride:
    def __init__(self, model, *args, **kwargs):
        self.model = model
        self.call_count = 0

    def custom_forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, attention_kwargs=None) -> torch.Tensor:
        self.call_count += 1
        return self.model.decoder(x, skip=skip, attention_kwargs=attention_kwargs)
    
    def register(self):
        self.model.decoder.override.custom_forward = self.custom_forward

class VAErmtokenControl:
    def __init__(self,
                 save_top5_path=None,
                 save_attn_cloud=True,
                 xyz_sample_mask=None,
                 token_k=1,
                 ):
        self.call_count = 0
        self.save_attn_cloud = save_attn_cloud
        self.save_top5_path = save_top5_path
        self.token_k = token_k  # Number of top tokens to keep
        # sample_mask: None for all points, or a boolean tensor
        self.xyz_sample_mask= xyz_sample_mask
        if self.xyz_sample_mask is None:
            print("ğŸ˜ºNo xyz_sample_mask provided, will use all points for attention distribution.")
        self.vae = None
        self.original_decode = None
        self.decoder_override = None
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†è³‡æºï¼Œæ¢å¾©åŸå§‹ç‹€æ…‹"""
        if self.vae is not None and self.original_decode is not None:
            # æ¢å¾©åŸå§‹çš„ _decode æ–¹æ³•
            self.vae._decode = self.original_decode

        
        if self.decoder_override is not None:
            # æ¸…ç† decoder override çš„è³‡æº
            self.decoder_override.vis_xyz_prob = []
            self.decoder_override.vis_xyz_attn = []
            self.decoder_override.vis_xyz_token_indices = []

        
        # æ¸…ç†å¼•ç”¨
        self.vae = None
        self.original_decode = None
        self.decoder_override = None
        
        # æ¸…ç† GPU å¿«å–
        torch.cuda.empty_cache()

    
    # original call geometric_func = lambda x: self.vae.decode(latents, sampled_points=x).sample
    # decoded = self._decode(z, sampled_points, **kwargs).sample
    def register(self,vae):
        # ä¿å­˜ VAE å’ŒåŸå§‹ decode æ–¹æ³•çš„å¼•ç”¨
        self.vae = vae
        self.original_decode = vae._decode
        
        #self.model.decoder.override.custom_forward = self.custom_forward
        # Also overide the cross attn block of vae.decoder
        self.decoder_override = dec_cross_forward_override()
        self.decoder_override.register(vae.decoder.blocks[-1])
        
        @torch.no_grad()
        def custom_vae_decode(
            z: torch.Tensor,
            sampled_points: torch.Tensor,
            num_chunks: int = 50000,
            to_cpu: bool = False,
            return_dict: bool = True,
        ) -> Union[DecoderOutput, torch.Tensor]:
            self.call_count+=1
            xyz_samples = sampled_points
            grid_dim = round(xyz_samples.shape[1]**(1/3))
            z = vae.post_quant(z)        
            #print("z.shape after post_quant:", z.shape)

            num_points = xyz_samples.shape[1]
            kv_cache = None
            dec = []
            
            print(f"Starting to calculate token attention distribution for with latents = {z.shape}.")
            self.decoder_override.token_attention_distribution = torch.zeros((z.shape[1]), dtype=torch.float16, device=z.device)

            #for i in tqdm(range(0, num_points, num_chunks),total=num_points // num_chunks + 1, desc="Saving decoding point for visualization"):
                # provide current xyz information to the decoder
            for i in range(0, num_points, num_chunks):
                queries = xyz_samples[:, i : i + num_chunks, :].to(z.device, dtype=z.dtype)
                queries_mask = self.xyz_sample_mask[:, i : i + num_chunks] if self.xyz_sample_mask is not None else None
                # Currently not supoort batch queries_mask
                queries = vae.embedder(queries)
                self.decoder_override.queries_mask = queries_mask[0] if self.xyz_sample_mask is not None else None 
                z_, kv_cache = vae.decoder(z, queries, kv_cache)
                dec.append(z_ if not to_cpu else z_.cpu())

            z = torch.cat(dec, dim=1)

            #print(f"concatenating  overide attn score")
            out_prob_3d = torch.cat(
                [d for d in self.decoder_override.vis_xyz_prob], dim=0
            )
            out_prob_tokens = torch.cat(
                [d for d in self.decoder_override.vis_xyz_token_indices], dim=0
            )
            

            if self.save_top5_path is not None:
                plot_top5_prob_histogram(out_prob_3d.cpu().numpy(), num_bins=50, output_path=self.save_top5_path)
                plot_token_top5_counts(out_prob_tokens.cpu().numpy(), output_path=self.save_top5_path)
                np.save(f"{self.save_top5_path}_token.npy", out_prob_tokens.cpu().numpy().astype("int16"))
                np.save(f"{self.save_top5_path}_prob.npy", out_prob_3d.cpu().numpy().astype("float16"))

            self.top_token_indices = self.decoder_override.token_attention_distribution.topk(k=self.token_k, dim=0)
            print(f"Top token indices: {self.top_token_indices.indices}")

            
            # Release override attn score
            #print("realease override attn score")
            self.decoder_override.vis_xyz_prob = []
            self.decoder_override.vis_xyz_attn = []
            self.decoder_override.vis_xyz_token_indices=[]

            if not return_dict:
                return (z,)

            return DecoderOutput(sample=z)
        vae._decode=custom_vae_decode
            



def save_grid_logits_as_pointcloud(grid_logits: torch.Tensor, 
                                   output_path: str = "grid_logits_pointcloud",
                                   format_type: str = "ply",
                                   threshold: float = -5.0,
                                   downsample_factor: int = 1,
                                   colormap: str = "coolwarm"):
    """
    å°‡3D grid logitsè½‰æ›ç‚ºé»é›²æ ¼å¼ä¸¦å„²å­˜
    
    é©åˆçš„3Dé»é›²æ ¼å¼ï¼š
    1. PLY (Polygon File Format) - æœ€å¸¸ç”¨ï¼Œæ”¯æ´é¡è‰²
    2. PCD (Point Cloud Data) - PCL libraryæ ¼å¼
    3. XYZ - ç°¡å–®æ–‡å­—æ ¼å¼
    4. OFF - Object File Format
    
    Args:
        grid_logits: shape (H, W, D) çš„3D tensor
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
        format_type: æ ¼å¼é¡å‹ ("ply", "xyz", "pcd")
        threshold: åªä¿å­˜å¤§æ–¼æ­¤é–¾å€¼çš„é»ï¼ˆéæ¿¾èƒŒæ™¯ï¼‰
        downsample_factor: ä¸‹æ¡æ¨£å› å­ï¼Œæ¸›å°‘é»çš„æ•¸é‡
        colormap: è‰²å½©æ˜ å°„é¡å‹ ("viridis", "plasma", "inferno", "magma", "hot", "coolwarm", "RdYlBu_r")
    """
    
    if isinstance(grid_logits, torch.Tensor):
        grid_logits = grid_logits.cpu().numpy()
    
    H, W, D = grid_logits.shape
    print(f"Original grid shape: {grid_logits.shape}")
    print(f"Value range: {grid_logits.min():.3f} to {grid_logits.max():.3f}")
    
    # ç”Ÿæˆ3Dåº§æ¨™ç¶²æ ¼
    x_coords, y_coords, z_coords = np.meshgrid(
        np.arange(0, H, downsample_factor),
        np.arange(0, W, downsample_factor), 
        np.arange(0, D, downsample_factor),
        indexing='ij'
    )
    
    # ä¸‹æ¡æ¨£grid_logits
    downsampled_logits = grid_logits[::downsample_factor, ::downsample_factor, ::downsample_factor]
    
    # æ‰å¹³åŒ–æ‰€æœ‰æ•¸æ“š
    points = np.stack([x_coords.flatten(), y_coords.flatten(), z_coords.flatten()], axis=1)
    values = downsampled_logits.flatten()
    
    # æ ¹æ“šé–¾å€¼éæ¿¾é»
    mask = values > threshold
    filtered_points = points[mask]
    filtered_values = values[mask]
    
    print(f"Filtered points: {len(filtered_points)} / {len(points)}")
    
    # å°‡æ•¸å€¼æ˜ å°„åˆ°é¡è‰²ï¼ˆä½¿ç”¨matplotlib heatmapé¡è‰²æ˜ å°„ï¼‰
    # æ­£è¦åŒ–åˆ°0-1ç¯„åœ
    min_val, max_val = filtered_values.min(), filtered_values.max()
    normalized_values = (filtered_values - min_val) / (max_val - min_val) if max_val > min_val else np.zeros_like(filtered_values)
    
    # ä½¿ç”¨matplotlib viridisè‰²å½©æ˜ å°„ (æ·±è—->ç¶ ->é»ƒ->ç´…)
    # ä¹Ÿå¯ä»¥é¸æ“‡å…¶ä»–è‰²å½©æ˜ å°„: 'plasma', 'inferno', 'magma', 'cividis', 'hot', 'coolwarm'
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    
    # é¸æ“‡è‰²å½©æ˜ å°„ - æä¾›å¤šç¨®heatmapé¡è‰²é¸æ“‡
    colormap_obj = cm.get_cmap(colormap)
    
    # å°‡æ­£è¦åŒ–å€¼æ˜ å°„åˆ°RGBé¡è‰²
    colors_rgba = colormap_obj(normalized_values)
    colors = (colors_rgba[:, :3] * 255).astype(np.uint8)  # è½‰æ›ç‚º0-255ç¯„åœçš„RGB
    
    if format_type.lower() == "ply":
        output_file = f"{output_path}.ply"
        save_ply_pointcloud(filtered_points, colors, filtered_values, output_file)
    elif format_type.lower() == "xyz":
        output_file = f"{output_path}.xyz"
        save_xyz_pointcloud(filtered_points, colors, filtered_values, output_file)
    elif format_type.lower() == "pcd":
        output_file = f"{output_path}.pcd"
        save_pcd_pointcloud(filtered_points, colors, filtered_values, output_file)
    else:
        raise ValueError(f"Unsupported format: {format_type}")
    
    print(f"Point cloud saved to: {output_file}")
    return output_file

def save_ply_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºPLYæ ¼å¼ï¼ˆæœ€æ¨è–¦ï¼Œæ”¯æ´å¤šæ•¸3Dè»Ÿé«”ï¼‰"""
    with open(output_path, 'w') as f:
        # PLY header
        f.write("ply\n")
        f.write("format ascii 1.0\n")
        f.write(f"element vertex {len(points)}\n")
        f.write("property float x\n")
        f.write("property float y\n")
        f.write("property float z\n")
        f.write("property uchar red\n")
        f.write("property uchar green\n")
        f.write("property uchar blue\n")
        f.write("property float value\n")
        f.write("end_header\n")
        
        # é»æ•¸æ“š
        for i in range(len(points)):
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} "
                   f"{colors[i,0]} {colors[i,1]} {colors[i,2]} {values[i]:.6f}\n")

def save_xyz_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºXYZæ ¼å¼ï¼ˆç°¡å–®æ–‡å­—æ ¼å¼ï¼‰"""
    with open(output_path, 'w') as f:
        for i in range(len(points)):
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} "
                   f"{colors[i,0]} {colors[i,1]} {colors[i,2]} {values[i]:.6f}\n")

def save_pcd_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºPCDæ ¼å¼ï¼ˆPCL libraryæ ¼å¼ï¼‰"""
    with open(output_path, 'w') as f:
        # PCD header
        f.write("# .PCD v0.7 - Point Cloud Data file format\n")
        f.write("VERSION 0.7\n")
        f.write("FIELDS x y z rgb value\n")
        f.write("SIZE 4 4 4 4 4\n")
        f.write("TYPE F F F U F\n")
        f.write("COUNT 1 1 1 1 1\n")
        f.write(f"WIDTH {len(points)}\n")
        f.write("HEIGHT 1\n")
        f.write("VIEWPOINT 0 0 0 1 0 0 0\n")
        f.write(f"POINTS {len(points)}\n")
        f.write("DATA ascii\n")
        
        # é»æ•¸æ“š
        for i in range(len(points)):
            # å°‡RGBæ‰“åŒ…æˆå–®ä¸€æ•´æ•¸
            rgb = (int(colors[i,0]) << 16) | (int(colors[i,1]) << 8) | int(colors[i,2])
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} {rgb} {values[i]:.6f}\n")


def otsu_threshold_1d(data):
    """
    Otsu's method for 1D data to find optimal threshold
    
    Args:
        data: 1D numpy array of values
        
    Returns:
        optimal_threshold: the threshold value that maximizes inter-class variance
    """
    data = data.flatten()
    
    # Create histogram
    hist, bin_edges = np.histogram(data, bins=256)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    
    # Normalize histogram
    hist = hist.astype(np.float32)
    hist = hist / np.sum(hist)
    
    # Compute cumulative sums
    cumsum = np.cumsum(hist)
    cumsum_mean = np.cumsum(hist * bin_centers)
    
    # Avoid division by zero
    cumsum = np.maximum(cumsum, 1e-10)
    
    # Compute total mean
    global_mean = np.sum(hist * bin_centers)
    
    # Compute between-class variance for each possible threshold
    between_class_variance = np.zeros_like(cumsum)
    
    for i in range(len(cumsum)):
        if cumsum[i] > 0 and cumsum[i] < 1:
            # Mean of class 1 (below threshold)
            mean1 = cumsum_mean[i] / cumsum[i]
            
            # Mean of class 2 (above threshold)  
            w2 = 1 - cumsum[i]
            if w2 > 0:
                mean2 = (global_mean - cumsum_mean[i]) / w2
                
                # Between-class variance
                between_class_variance[i] = cumsum[i] * w2 * (mean1 - mean2) ** 2
    
    # Find threshold that maximizes between-class variance
    optimal_idx = np.argmax(between_class_variance)
    optimal_threshold = bin_centers[optimal_idx]
    
    return optimal_threshold


# Make Histogram of top5 prob
def plot_top5_prob_histogram(top5_probs, num_bins=50, output_path="./output/prob_histogram"):
    """
    ç¹ªè£½ top5 æ¦‚ç‡çš„ç›´æ–¹åœ–
    
    Args:
        top5_probs: 1D numpy array of top5 probabilities
        num_bins: Number of bins for the histogram
    """
    import matplotlib.pyplot as plt
    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)
    
    for k in range(5):
        plt.figure(figsize=(10, 6))
        plt.hist(top5_probs[:, k], bins=num_bins, color='blue', alpha=0.7)
        plt.title(f'Top {k+1} Probabilities Histogram')
        plt.xlabel('Probability')
        plt.ylabel('Frequency')
        plt.grid(True)
        plt.savefig(os.path.join(output_path, f'rank_{k+1}.png'))


def plot_token_top5_counts(top5_indices, output_path="./output/prob_histogram"):
    """
    ç¹ªè£½ top5 token çš„è¨ˆæ•¸ç›´æ–¹åœ–

    Args:
        top5_indices: 2D numpy array of top5 token indices
    """
    import matplotlib.pyplot as plt
    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)

    for k in range(5):
        plt.figure(figsize=(10, 6))
        plt.hist(top5_indices[:, k],bins=2048)

        plt.title(f'Top {k+1} Tokens counts')
        plt.xlabel('Token Index')
        plt.ylabel('Frequency')
        plt.grid(True)
        plt.savefig(os.path.join(output_path, f'token_count_rank={k+1}.png'))
