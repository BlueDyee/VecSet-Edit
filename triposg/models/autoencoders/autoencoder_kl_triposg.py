from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.models.attention_processor import Attention, AttentionProcessor
from diffusers.models.autoencoders.vae import DecoderOutput
from diffusers.models.modeling_outputs import AutoencoderKLOutput
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import FP32LayerNorm, LayerNorm
from diffusers.utils import logging
from diffusers.utils.accelerate_utils import apply_forward_hook
from einops import repeat
from torch_cluster import fps
from tqdm import tqdm

from ..attention_processor import FusedTripoSGAttnProcessor2_0, TripoSGAttnProcessor2_0, FlashTripoSGAttnProcessor2_0
from ..attention_processor import CustomAttnProcessor2_0
from ..embeddings import FrequencyPositionalEmbedding
from ..transformers.triposg_transformer import DiTBlock
from .vae import DiagonalGaussianDistribution

logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

DEBUG_PRINT = False  # Set to True to enable debug prints
print("ğŸ›ğŸ› DEBUG PRINT (autoencoder_kl_triposg.py)") if DEBUG_PRINT else None


def otsu_threshold_1d(data):
    """
    Otsu's method for 1D data to find optimal threshold
    
    Args:
        data: 1D numpy array of values
        
    Returns:
        optimal_threshold: the threshold value that maximizes inter-class variance
    """
    data = data.flatten()
    
    # Create histogram
    hist, bin_edges = np.histogram(data, bins=256)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    
    # Normalize histogram
    hist = hist.astype(np.float32)
    hist = hist / np.sum(hist)
    
    # Compute cumulative sums
    cumsum = np.cumsum(hist)
    cumsum_mean = np.cumsum(hist * bin_centers)
    
    # Avoid division by zero
    cumsum = np.maximum(cumsum, 1e-10)
    
    # Compute total mean
    global_mean = np.sum(hist * bin_centers)
    
    # Compute between-class variance for each possible threshold
    between_class_variance = np.zeros_like(cumsum)
    
    for i in range(len(cumsum)):
        if cumsum[i] > 0 and cumsum[i] < 1:
            # Mean of class 1 (below threshold)
            mean1 = cumsum_mean[i] / cumsum[i]
            
            # Mean of class 2 (above threshold)  
            w2 = 1 - cumsum[i]
            if w2 > 0:
                mean2 = (global_mean - cumsum_mean[i]) / w2
                
                # Between-class variance
                between_class_variance[i] = cumsum[i] * w2 * (mean1 - mean2) ** 2
    
    # Find threshold that maximizes between-class variance
    optimal_idx = np.argmax(between_class_variance)
    optimal_threshold = bin_centers[optimal_idx]
    
    return optimal_threshold


class TripoSGEncoder(nn.Module):
    def __init__(
        self,
        in_channels: int = 3,
        dim: int = 512,
        num_attention_heads: int = 8,
        num_layers: int = 8,
    ):
        super().__init__()

        self.proj_in = nn.Linear(in_channels, dim, bias=True)

        self.blocks = nn.ModuleList(
            [
                DiTBlock(
                    dim=dim,
                    num_attention_heads=num_attention_heads,
                    use_self_attention=False,
                    use_cross_attention=True,
                    cross_attention_dim=dim,
                    cross_attention_norm_type="layer_norm",
                    activation_fn="gelu",
                    norm_type="fp32_layer_norm",
                    norm_eps=1e-5,
                    qk_norm=False,
                    qkv_bias=False,
                )  # cross attention
            ]
            + [
                DiTBlock(
                    dim=dim,
                    num_attention_heads=num_attention_heads,
                    use_self_attention=True,
                    self_attention_norm_type="fp32_layer_norm",
                    use_cross_attention=False,
                    activation_fn="gelu",
                    norm_type="fp32_layer_norm",
                    norm_eps=1e-5,
                    qk_norm=False,
                    qkv_bias=False,
                )
                for _ in range(num_layers)  # self attention
            ]
        )

        self.norm_out = LayerNorm(dim)

    def forward(self, sample_1: torch.Tensor, sample_2: torch.Tensor):
        hidden_states = self.proj_in(sample_1)
        encoder_hidden_states = self.proj_in(sample_2)

        for layer, block in enumerate(self.blocks):
            if layer == 0:
                hidden_states = block(
                    hidden_states, encoder_hidden_states=encoder_hidden_states
                )
            else:
                hidden_states = block(hidden_states)

        hidden_states = self.norm_out(hidden_states)

        return hidden_states


class dec_cross_forward_override:
    def __init__(self, vis_token=0):
        self.store_information = {"score_map": None, "attn_map": None}
        self.attn2_processor = CustomAttnProcessor2_0()
        self.grid_matrix= None
        self.xyz_queries = None
        self.vis_token = vis_token  # ç”¨æ–¼å¯è¦–åŒ–çš„ tokenï¼Œé»˜èªç‚º 0
        self.vis_xyz_val=[]
        self.call_count=0
        # self.rescale_xyz_metrics = torch.linspace(-1.0049, 1.0049, int(256), dtype=torch.float16)

    def register(self, block):
        # overwrite block.attn2.preocessor
        block.attn2.set_processor(self.attn2_processor)
        # ä½¿ç”¨é–‰åŒ…æ•ç² selfï¼ˆoverride å¯¦ä¾‹)
        def custom_forward(
            hidden_states: torch.Tensor,
            encoder_hidden_states: Optional[torch.Tensor] = None,
            encoder_hidden_states_2: Optional[torch.Tensor] = None,
            temb: Optional[torch.Tensor] = None,
            image_rotary_emb: Optional[torch.Tensor] = None,
            skip: Optional[torch.Tensor] = None,
            attention_kwargs= None,
            ) -> torch.Tensor:
            self.call_count+=1
            assert isinstance(self.grid_matrix, torch.Tensor), "grid_matrix must be set before calling custom_forward"
            assert isinstance(self.xyz_queries, torch.Tensor), "xyz_queries must be set before calling custom_forward"
            if self.store_information.get("score_map") is None:
                self.store_information["score_map"] = self.grid_matrix.clone()
                print("ğŸ› in custom_forward creating 3d attn map:", self.store_information.get("score_map").shape)
            if self.store_information.get("attn_map") is None:
                self.store_information["attn_map"] = self.grid_matrix.clone()

            # é€™è£¡å¯ä»¥åŒæ™‚è¨ªå• selfï¼ˆoverride å¯¦ä¾‹ï¼‰å’Œ block
            #self.store_information["hidden_states"] = hidden_states
        
            # Prepare attention kwargs
            attention_kwargs = attention_kwargs.copy() if attention_kwargs is not None else {}
            cross_attention_scale = attention_kwargs.pop("cross_attention_scale", 1.0)
            cross_attention_2_scale = attention_kwargs.pop("cross_attention_2_scale", 1.0)

            # Notice that normalization is always applied before the real computation in the following blocks.
            # 0. Long Skip Connection
            if block.skip_linear is not None:
                cat = torch.cat(
                    (
                        [skip, hidden_states]
                        if block.skip_concat_front
                        else [hidden_states, skip]
                    ),
                    dim=-1,
                )
                if block.skip_norm_last:
                    # don't do this
                    hidden_states = block.skip_linear(cat)
                    hidden_states = block.skip_norm(hidden_states)
                else:
                    cat = block.skip_norm(cat)
                    hidden_states = block.skip_linear(cat)

            # 1. Self-Attention
            if block.use_self_attention:
                norm_hidden_states = block.norm1(hidden_states)
                attn_output = block.attn1(
                    norm_hidden_states,
                    image_rotary_emb=image_rotary_emb,
                    **attention_kwargs,
                )
                hidden_states = hidden_states + attn_output

            # 2. Cross-Attention
            if block.use_cross_attention:
                if block.use_cross_attention_2:
                    hidden_states = (
                        hidden_states
                        + block.attn2(
                            block.norm2(hidden_states),
                            encoder_hidden_states=encoder_hidden_states,
                            image_rotary_emb=image_rotary_emb,
                            **attention_kwargs,
                        ) * cross_attention_scale
                        + block.attn2_2(
                            block.norm2_2(hidden_states),
                            encoder_hidden_states=encoder_hidden_states_2,
                            image_rotary_emb=image_rotary_emb,
                            **attention_kwargs,
                        ) * cross_attention_2_scale
                    )
                else:
                    # This attn2 is overide by custom_attention processor
                    attn_output= block.attn2(
                        block.norm2(hidden_states),
                        encoder_hidden_states=encoder_hidden_states,
                        image_rotary_emb=image_rotary_emb,
                        **attention_kwargs,
                    ) * cross_attention_scale
                    hidden_states = hidden_states + attn_output

                    if DEBUG_PRINT:
                        attn_probs = self.attn2_processor.attn_probs
                        attn_probs_val=attn_probs[0].mean(dim=0)
                        # print("ğŸ› in custom_forward xyz_queries output:", self.xyz_queries.shape)
                        self.vis_xyz_val.append(attn_probs_val[:,self.vis_token].cpu())
                        # for xyz_index, xyz in enumerate(self.xyz_queries[0]):
                        #     print("xyz", xyz)
                        #     self.store_information["attn_map"][xyz[0], xyz[1], xyz[2]] = attn_probs_val[xyz_index, self.vis_token]

            # FFN Layer ### TODO: switch norm2 and norm3 in the state dict
            mlp_inputs = block.norm3(hidden_states)
            hidden_states = hidden_states + block.ff(mlp_inputs)

            return hidden_states
            
        # ç›´æ¥è³¦å€¼ï¼Œä¸éœ€è¦ __get__
        block.forward = custom_forward

class TripoSGDecoder(nn.Module):
    def __init__(
        self,
        in_channels: int = 3,  # è¼¸å…¥æŸ¥è©¢é»çš„ç¶­åº¦ï¼ˆé€šå¸¸æ˜¯ xyz åº§æ¨™ï¼‰
        out_channels: int = 1,  # è¼¸å‡ºç¶­åº¦ï¼ˆé€šå¸¸æ˜¯ SDF å€¼ï¼‰
        dim: int = 512,  # æ¨¡å‹çš„éš±è—ç¶­åº¦
        num_attention_heads: int = 8,  # æ³¨æ„åŠ›é ­æ•¸
        num_layers: int = 16,  # è‡ªæ³¨æ„åŠ›å±¤æ•¸
        grad_type: str = "analytical",  # æ¢¯åº¦è¨ˆç®—æ–¹å¼ï¼š"numerical" æˆ– "analytical"
        grad_interval: float = 0.001,  # æ•¸å€¼æ¢¯åº¦çš„é–“éš”
        debug_print: bool = DEBUG_PRINT,  # æ˜¯å¦æ‰“å°èª¿è©¦ä¿¡æ¯
    ):
        super().__init__()
        self.debug_print = debug_print
        if grad_type not in ["numerical", "analytical"]:
            raise ValueError(f"grad_type must be one of ['numerical', 'analytical']")
        self.grad_type = grad_type
        
        self.grad_interval = grad_interval

        # æ§‹å»º Transformer å¡Šï¼šå‰ num_layers å€‹æ˜¯è‡ªæ³¨æ„åŠ›ï¼Œæœ€å¾Œä¸€å€‹æ˜¯äº¤å‰æ³¨æ„åŠ›
        self.blocks = nn.ModuleList(
            [
                DiTBlock(
                    dim=dim,
                    num_attention_heads=num_attention_heads,
                    use_self_attention=True,  # ä½¿ç”¨è‡ªæ³¨æ„åŠ›é€²è¡Œç‰¹å¾µæå–
                    self_attention_norm_type="fp32_layer_norm",
                    use_cross_attention=False,
                    activation_fn="gelu",
                    norm_type="fp32_layer_norm",
                    norm_eps=1e-5,
                    qk_norm=False,
                    qkv_bias=False,
                )
                for _ in range(num_layers)  # å¤šå±¤è‡ªæ³¨æ„åŠ›ç”¨æ–¼è™•ç† latent features
            ]
            + [
                DiTBlock(
                    dim=dim,
                    num_attention_heads=num_attention_heads,
                    use_self_attention=False,
                    use_cross_attention=True,  # æœ€å¾Œä¸€å±¤ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œç‚ºäº†å¯ä»¥è·Ÿgrid query compute geometry
                    cross_attention_dim=dim,
                    cross_attention_norm_type="layer_norm",
                    activation_fn="gelu",
                    norm_type="fp32_layer_norm",
                    norm_eps=1e-5,
                    qk_norm=False,
                    qkv_bias=False,
                )  # äº¤å‰æ³¨æ„åŠ›ç”¨æ–¼æŸ¥è©¢ç‰¹å®šä½ç½®çš„å¹¾ä½•è³‡è¨Š
            ]
        )

        if self.debug_print:
            print(f"ğŸ›â€¼ï¸ Debug mode (TripoSGDecoder): Using custom forward for last block")
            self.override = dec_cross_forward_override()
            self.override.register(self.blocks[-1])

        # å°‡æŸ¥è©¢é»æŠ•å½±åˆ°æ¨¡å‹ç¶­åº¦
        self.proj_query = nn.Linear(in_channels, dim, bias=True)

        # è¼¸å‡ºå±¤
        self.norm_out = LayerNorm(dim)
        self.proj_out = nn.Linear(dim, out_channels, bias=True)


    def set_topk(self, topk):
        """è¨­å®šæœ€å¾Œä¸€å±¤äº¤å‰æ³¨æ„åŠ›çš„ top-k åƒæ•¸ä»¥ç¯€çœè¨˜æ†¶é«”"""
        self.blocks[-1].set_topk(topk)

    def set_flash_processor(self, processor):
        """è¨­å®š Flash Attention è™•ç†å™¨ä»¥åŠ é€Ÿè¨ˆç®—"""
        self.blocks[-1].set_flash_processor(processor)

    def query_geometry(
        self,
        model_fn: callable,
        queries: torch.Tensor,  # æŸ¥è©¢é»åº§æ¨™ (B, N, 3)
        sample: torch.Tensor,   # ç·¨ç¢¼å¾Œçš„æ½›åœ¨ç‰¹å¾µ (B, M, dim)
        grad: bool = False,     # æ˜¯å¦è¨ˆç®—æ¢¯åº¦
    ):
        """
        æŸ¥è©¢ç‰¹å®šä½ç½®çš„å¹¾ä½•è³‡è¨Šï¼ˆå¦‚ SDF å€¼ï¼‰
        
        Args:
            model_fn: æ¨¡å‹å‡½æ•¸ï¼Œæ¥å—æŸ¥è©¢é»å’Œæ½›åœ¨ç‰¹å¾µï¼Œè¿”å›é æ¸¬å€¼
            queries: æŸ¥è©¢é»åº§æ¨™
            sample: ç·¨ç¢¼å¾Œçš„æ½›åœ¨ç‰¹å¾µ
            grad: æ˜¯å¦è¨ˆç®—æ¢¯åº¦ï¼ˆç”¨æ–¼è¡¨é¢æ³•å‘é‡è¨ˆç®—ï¼‰
        
        Returns:
            logits: é æ¸¬çš„å¹¾ä½•å€¼
            grad_value: æ¢¯åº¦å€¼ï¼ˆå¦‚æœ grad=Trueï¼‰
        """
        # ç²å–åŸºæœ¬çš„å¹¾ä½•é æ¸¬å€¼
        logits = model_fn(queries, sample)
        
        if grad:
            # è¨ˆç®—æ¢¯åº¦ç”¨æ–¼è¡¨é¢æ³•å‘é‡æˆ–å…¶ä»–å¹¾ä½•è³‡è¨Š
            with torch.autocast(device_type="cuda", dtype=torch.float32):
                if self.grad_type == "numerical":
                    # æ•¸å€¼æ¢¯åº¦ï¼šä½¿ç”¨æœ‰é™å·®åˆ†æ³•
                    interval = self.grad_interval
                    grad_value = []
                    for offset in [
                        (interval, 0, 0),  # x æ–¹å‘
                        (0, interval, 0),  # y æ–¹å‘
                        (0, 0, interval),  # z æ–¹å‘
                    ]:
                        offset_tensor = torch.tensor(offset, device=queries.device)[
                            None, :
                        ]
                        # è¨ˆç®—æ­£å‘å’Œè² å‘çš„å‡½æ•¸å€¼
                        res_p = model_fn(queries + offset_tensor, sample)[..., 0]
                        res_n = model_fn(queries - offset_tensor, sample)[..., 0]
                        # è¨ˆç®—æ¢¯åº¦
                        grad_value.append((res_p - res_n) / (2 * interval))
                    grad_value = torch.stack(grad_value, dim=-1)
                else:
                    # è§£ææ¢¯åº¦ï¼šä½¿ç”¨è‡ªå‹•å¾®åˆ†
                    queries_d = torch.clone(queries)
                    queries_d.requires_grad = True
                    with torch.enable_grad():
                        res_d = model_fn(queries_d, sample)
                        grad_value = torch.autograd.grad(
                            res_d,
                            [queries_d],
                            grad_outputs=torch.ones_like(res_d),
                            create_graph=self.training,  # è¨“ç·´æ™‚ä¿ç•™è¨ˆç®—åœ–
                        )[0]
        else:
            grad_value = None

        return logits, grad_value

    def forward(
        self,
        sample: torch.Tensor,     # æ½›åœ¨ç‰¹å¾µ (B, M, dim)
        queries: torch.Tensor,    # æŸ¥è©¢é»åº§æ¨™ (B, N, 3)
        kv_cache: Optional[torch.Tensor] = None,  # å¿«å–çš„éµå€¼å°
    ):
        """
        Vec2Set Decoder çš„å‰å‘å‚³æ’­
        
        å·¥ä½œæµç¨‹ï¼š
        1. ä½¿ç”¨è‡ªæ³¨æ„åŠ›å±¤è™•ç†æ½›åœ¨ç‰¹å¾µï¼Œæå–å…¨å±€å¹¾ä½•è³‡è¨Š
        2. ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æŸ¥è©¢ç‰¹å®šä½ç½®çš„å¹¾ä½•å€¼
        3. å¯é¸åœ°è¨ˆç®—æ¢¯åº¦ç”¨æ–¼è¡¨é¢æ³•å‘é‡
        
        Args:
            sample: ä¾†è‡ªç·¨ç¢¼å™¨çš„æ½›åœ¨ç‰¹å¾µ
            queries: è¦æŸ¥è©¢çš„ 3D åº§æ¨™é»
            kv_cache: å¿«å–çš„ç‰¹å¾µï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
        
        Returns:
            logits: æŸ¥è©¢é»çš„å¹¾ä½•é æ¸¬å€¼
            kv_cache: è™•ç†å¾Œçš„ç‰¹å¾µå¿«å–
        """
        if kv_cache is None:
            
            # ç¬¬ä¸€æ¬¡èª¿ç”¨ï¼šé€šéè‡ªæ³¨æ„åŠ›å±¤è™•ç†æ½›åœ¨ç‰¹å¾µ
            # å› ç‚ºæˆ‘å€‘æ²’è¾¦æ³•ä¸€æ¬¡è™•è£¡æ‰€æœ‰çš„grid pointsï¼Œæ‰€ä»¥éœ€è¦åˆ†æ‰¹è™•ç†
            # ä½†æ˜¯æ½›åœ¨ç‰¹å¾µéƒ½æ˜¯ä¸€æ¨£çš„ï¼Œæ‰€ä»¥æˆ‘å€‘åªéœ€è¦forward æ½›åœ¨ç‰¹å¾µ with self-attention
            hidden_states = sample
            for _, block in enumerate(self.blocks[:-1]):
                # ä½¿ç”¨è‡ªæ³¨æ„åŠ›æå–å’Œæ•´åˆå…¨å±€å¹¾ä½•ç‰¹å¾µ
                hidden_states = block(hidden_states)
            kv_cache = hidden_states  # å¿«å–è™•ç†å¾Œçš„ç‰¹å¾µ

    

        # å®šç¾©æŸ¥è©¢å‡½æ•¸ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æŸ¥è©¢ç‰¹å®šä½ç½®çš„å¹¾ä½•è³‡è¨Š
        def query_fn(q, kv):
            # å°‡æŸ¥è©¢é»æŠ•å½±åˆ°æ¨¡å‹ç¶­åº¦
            q = self.proj_query(q)
            # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼šæŸ¥è©¢é»ä½œç‚º queryï¼Œæ½›åœ¨ç‰¹å¾µä½œç‚º key å’Œ value
            l = self.blocks[-1](q, encoder_hidden_states=kv)
            # è¼¸å‡ºå¹¾ä½•é æ¸¬å€¼
            return self.proj_out(self.norm_out(l))
        
        if self.debug_print:
            def query_fn(q,kv):
                # å°‡æŸ¥è©¢é»æŠ•å½±åˆ°æ¨¡å‹ç¶­åº¦
                q = self.proj_query(q)
                # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼šæŸ¥è©¢é»ä½œç‚º queryï¼Œæ½›åœ¨ç‰¹å¾µä½œç‚º key å’Œ value
                l = self.blocks[-1](q, encoder_hidden_states=kv)
                # è¼¸å‡ºå¹¾ä½•é æ¸¬å€¼
                return self.proj_out(self.norm_out(l)), None

        # åŸ·è¡Œå¹¾ä½•æŸ¥è©¢
        logits, grad = self.query_geometry(
            query_fn, queries, kv_cache, grad=self.training
        )
        
        # å°‡ logits å–è² å€¼ï¼ˆå¯èƒ½æ˜¯å› ç‚ºä½¿ç”¨ SDF çš„æ…£ä¾‹ï¼‰
        logits = logits * -1 if not isinstance(logits, Tuple) else logits[0] * -1

        return logits, kv_cache


class TripoSGVAEModel(ModelMixin, ConfigMixin):
    @register_to_config
    def __init__(
        self,
        in_channels: int = 3,  # NOTE xyz instead of feature dim
        latent_channels: int = 64,
        num_attention_heads: int = 8,
        width_encoder: int = 512,
        width_decoder: int = 1024,
        num_layers_encoder: int = 8,
        num_layers_decoder: int = 16,
        embedding_type: str = "frequency",
        embed_frequency: int = 8,
        embed_include_pi: bool = False,
    ):
        super().__init__()

        self.out_channels = 1

        if embedding_type == "frequency":
            self.embedder = FrequencyPositionalEmbedding(
                num_freqs=embed_frequency,
                logspace=True,
                input_dim=in_channels,
                include_pi=embed_include_pi,
            )
        else:
            raise NotImplementedError(
                f"Embedding type {embedding_type} is not supported."
            )

        self.encoder = TripoSGEncoder(
            in_channels=in_channels + self.embedder.out_dim,
            dim=width_encoder,
            num_attention_heads=num_attention_heads,
            num_layers=num_layers_encoder,
        )
        self.decoder = TripoSGDecoder(
            in_channels=self.embedder.out_dim,
            out_channels=self.out_channels,
            dim=width_decoder,
            num_attention_heads=num_attention_heads,
            num_layers=num_layers_decoder,
        )

        self.quant = nn.Linear(width_encoder, latent_channels * 2, bias=True)
        self.post_quant = nn.Linear(latent_channels, width_decoder, bias=True)

        self.use_slicing = False
        self.slicing_length = 1

    def set_flash_decoder(self):
        self.decoder.set_flash_processor(FlashTripoSGAttnProcessor2_0())

    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections with FusedAttnProcessor2_0->FusedTripoSGAttnProcessor2_0
    def fuse_qkv_projections(self):
        """
        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)
        are fused. For cross-attention modules, key and value projection matrices are fused.

        <Tip warning={true}>

        This API is ğŸ§ª experimental.

        </Tip>
        """
        self.original_attn_processors = None

        for _, attn_processor in self.attn_processors.items():
            if "Added" in str(attn_processor.__class__.__name__):
                raise ValueError(
                    "`fuse_qkv_projections()` is not supported for models having added KV projections."
                )

        self.original_attn_processors = self.attn_processors

        for module in self.modules():
            if isinstance(module, Attention):
                module.fuse_projections(fuse=True)

        self.set_attn_processor(FusedTripoSGAttnProcessor2_0())

    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
    def unfuse_qkv_projections(self):
        """Disables the fused QKV projection if enabled.

        <Tip warning={true}>

        This API is ğŸ§ª experimental.

        </Tip>

        """
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)

    @property
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors
    def attn_processors(self) -> Dict[str, AttentionProcessor]:
        r"""
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        """
        # set recursively
        processors = {}

        def fn_recursive_add_processors(
            name: str,
            module: torch.nn.Module,
            processors: Dict[str, AttentionProcessor],
        ):
            if hasattr(module, "get_processor"):
                processors[f"{name}.processor"] = module.get_processor()

            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

            return processors

        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)

        return processors

    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor
    def set_attn_processor(
        self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]
    ):
        r"""
        Sets the attention processor to use to compute attention.

        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.

                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.

        """
        count = len(self.attn_processors.keys())

        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
                f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
            )

        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            if hasattr(module, "set_processor"):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f"{name}.processor"))

            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)

    def set_default_attn_processor(self):
        """
        Disables custom attention processors and sets the default attention implementation.
        """
        self.set_attn_processor(TripoSGAttnProcessor2_0())

    def enable_slicing(self, slicing_length: int = 1) -> None:
        r"""
        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to
        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.
        """
        self.use_slicing = True
        self.slicing_length = slicing_length

    def disable_slicing(self) -> None:
        r"""
        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing
        decoding in one step.
        """
        self.use_slicing = False

    def _sample_features(
        self, x: torch.Tensor, num_tokens: int = 2048, seed: Optional[int] = None
    ):
        """
        Sample points from features of the input point cloud.

        Args:
            x (torch.Tensor): The input point cloud. shape: (B, N, C)
            num_tokens (int, optional): The number of points to sample. Defaults to 2048.
            seed (Optional[int], optional): The random seed. Defaults to None.
        """
        rng = np.random.default_rng(seed)
        indices = rng.choice(
            x.shape[1], num_tokens * 4, replace=num_tokens * 4 > x.shape[1]
        )
        selected_points = x[:, indices]

        batch_size, num_points, num_channels = selected_points.shape
        flattened_points = selected_points.view(batch_size * num_points, num_channels)
        batch_indices = (
            torch.arange(batch_size).to(x.device).repeat_interleave(num_points)
        )

        # fps sampling with fallback for CUDA compatibility issues
        sampling_ratio = 1.0 / 4
        
        sampled_indices = fps(
            flattened_points[:, :3],
            batch_indices,
            ratio=sampling_ratio,
            random_start=self.training,
        )
        sampled_points = flattened_points[sampled_indices].view(
            batch_size, -1, num_channels
        )

        return sampled_points

    def _encode(
        self, x: torch.Tensor, num_tokens: int = 2048, seed: Optional[int] = None
    ):
        position_channels = self.config.in_channels
        positions, features = x[..., :position_channels], x[..., position_channels:]
        x_kv = torch.cat([self.embedder(positions), features], dim=-1)
        sampled_x = self._sample_features(x, num_tokens, seed)
        positions, features = (
            sampled_x[..., :position_channels],
            sampled_x[..., position_channels:],
        )

        x_q = torch.cat([self.embedder(positions), features], dim=-1)

        x = self.encoder(x_q, x_kv)


        x = self.quant(x)

        return x

    @apply_forward_hook
    def encode(
        self, x: torch.Tensor, return_dict: bool = True, **kwargs
    ) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:
        """
        Encode a batch of point features into latents.
        """
        if self.use_slicing and x.shape[0] > 1:
            encoded_slices = [
                self._encode(x_slice, **kwargs)
                for x_slice in x.split(self.slicing_length)
            ]
            h = torch.cat(encoded_slices)
        else:
            h = self._encode(x, **kwargs)

        posterior = DiagonalGaussianDistribution(h, feature_dim=-1)

        if not return_dict:
            return (posterior,)
        return AutoencoderKLOutput(latent_dist=posterior)

    def _decode(
        self,
        z: torch.Tensor,
        sampled_points: torch.Tensor,
        num_chunks: int = 50000,
        to_cpu: bool = False,
        return_dict: bool = True,
        debug_print=DEBUG_PRINT,
    ) -> Union[DecoderOutput, torch.Tensor]:
        
        xyz_samples = sampled_points
        if debug_print:
            print("ğŸ› Debug mode (_decode)")if debug_print else None
            print("xyz_samples.shape:", xyz_samples.shape) if debug_print else None
            print("z.shape before post_quant:", z.shape) if debug_print else None
            grid_dim = round(xyz_samples.shape[1]**(1/3))
            #print("xyz_samples.shape:", xyz_samples.shape) if debug_print else None
            self.decoder.override.grid_matrix=torch.zeros(
                grid_dim, grid_dim, grid_dim, device=xyz_samples.device
            )
            print("grid_dim:", grid_dim) if debug_print else None
        z = self.post_quant(z)        

        num_points = xyz_samples.shape[1]
        kv_cache = None
        dec = []

        for i in range(0, num_points, num_chunks):
            # provide current xyz information to the decoder
            # self.decoder.override.xyz_queries=xyz_samples[:, i : i + num_chunks, :].to(z.device, dtype=z.dtype)
            queries = xyz_samples[:, i : i + num_chunks, :].to(z.device, dtype=z.dtype)
            queries = self.embedder(queries)
            z_, kv_cache = self.decoder(z, queries, kv_cache)
            dec.append(z_ if not to_cpu else z_.cpu())

        z = torch.cat(dec, dim=1)
        if debug_print:
            print(f"concatenating  overide attn score")
            out_attn_probs = torch.cat(
                [d for d in self.decoder.override.vis_xyz_val], dim=0
            )
            print(f"out_attn_probs: {out_attn_probs.shape}")
            
            # Apply Otsu's thresholding to find optimal cutting point
            otsu_threshold = otsu_threshold_1d(out_attn_probs.cpu().numpy())
            print(f"ğŸ” Otsu optimal threshold: {otsu_threshold:.6f}")
            
            # Apply threshold to create binary mask
            binary_mask = out_attn_probs > otsu_threshold
            num_positive = binary_mask.sum().item()
            total_points = out_attn_probs.numel()
            print(f"ğŸ“Š Points above threshold: {num_positive}/{total_points} ({100*num_positive/total_points:.2f}%)")
            
            grid_dim = round(xyz_samples.shape[1]**(1/3))
            out_attn_probs = out_attn_probs.to(torch.float16).view(grid_dim, grid_dim, grid_dim)
            print("ğŸ›ğŸ‘½Visualizing xyz attention map with shape:", out_attn_probs.shape)
            print("max, min attn probs:", out_attn_probs.max(), out_attn_probs.min())
            save_grid_logits_as_pointcloud(
                grid_logits=out_attn_probs,
                output_path=f"./output/attn_cloud/demo_{self.decoder.override.call_count}",
                format_type="ply",
                threshold=otsu_threshold,  # Use Otsu threshold instead of fixed value
                downsample_factor=2,  # ä¸‹æ¡æ¨£æ¸›å°‘é»æ•¸é‡
                colormap="coolwarm" 
            )
            
            # Also save binary mask as point cloud for visualization
            binary_mask_3d = binary_mask.to(torch.float16).view(grid_dim, grid_dim, grid_dim)
            save_grid_logits_as_pointcloud(
                grid_logits=binary_mask_3d,
                output_path=f"./output/attn_cloud/demo_binary_{self.decoder.override.call_count}",
                format_type="ply",
                threshold=0.5,  # Binary threshold
                downsample_factor=2,
                colormap="RdBu_r"  # Red-Blue colormap for binary visualization
            )
            # Release override attn score
            print("realease override attn score")
            self.decoder.override.vis_xyz_val = []

        if not return_dict:
            return (z,)

        return DecoderOutput(sample=z)

    @apply_forward_hook
    def decode(
        self,
        z: torch.Tensor,
        sampled_points: torch.Tensor,
        return_dict: bool = True,
        **kwargs,
    ) -> Union[DecoderOutput, torch.Tensor]:
        if self.use_slicing and z.shape[0] > 1:
            decoded_slices = [
                self._decode(z_slice, p_slice, **kwargs).sample
                for z_slice, p_slice in zip(
                    z.split(self.slicing_length),
                    sampled_points.split(self.slicing_length),
                )
            ]
            decoded = torch.cat(decoded_slices)
        else:
            decoded = self._decode(z, sampled_points, **kwargs).sample

        if not return_dict:
            return (decoded,)
        return DecoderOutput(sample=decoded)

    def forward(self, x: torch.Tensor):
        pass



def save_grid_logits_as_pointcloud(grid_logits: torch.Tensor, 
                                   output_path: str = "grid_logits_pointcloud",
                                   format_type: str = "ply",
                                   threshold: float = -5.0,
                                   downsample_factor: int = 1,
                                   colormap: str = "coolwarm"):
    """
    å°‡3D grid logitsè½‰æ›ç‚ºé»é›²æ ¼å¼ä¸¦å„²å­˜
    
    é©åˆçš„3Dé»é›²æ ¼å¼ï¼š
    1. PLY (Polygon File Format) - æœ€å¸¸ç”¨ï¼Œæ”¯æ´é¡è‰²
    2. PCD (Point Cloud Data) - PCL libraryæ ¼å¼
    3. XYZ - ç°¡å–®æ–‡å­—æ ¼å¼
    4. OFF - Object File Format
    
    Args:
        grid_logits: shape (H, W, D) çš„3D tensor
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
        format_type: æ ¼å¼é¡å‹ ("ply", "xyz", "pcd")
        threshold: åªä¿å­˜å¤§æ–¼æ­¤é–¾å€¼çš„é»ï¼ˆéæ¿¾èƒŒæ™¯ï¼‰
        downsample_factor: ä¸‹æ¡æ¨£å› å­ï¼Œæ¸›å°‘é»çš„æ•¸é‡
        colormap: è‰²å½©æ˜ å°„é¡å‹ ("viridis", "plasma", "inferno", "magma", "hot", "coolwarm", "RdYlBu_r")
    """
    
    if isinstance(grid_logits, torch.Tensor):
        grid_logits = grid_logits.cpu().numpy()
    
    H, W, D = grid_logits.shape
    print(f"Original grid shape: {grid_logits.shape}")
    print(f"Value range: {grid_logits.min():.3f} to {grid_logits.max():.3f}")
    
    # ç”Ÿæˆ3Dåº§æ¨™ç¶²æ ¼
    x_coords, y_coords, z_coords = np.meshgrid(
        np.arange(0, H, downsample_factor),
        np.arange(0, W, downsample_factor), 
        np.arange(0, D, downsample_factor),
        indexing='ij'
    )
    
    # ä¸‹æ¡æ¨£grid_logits
    downsampled_logits = grid_logits[::downsample_factor, ::downsample_factor, ::downsample_factor]
    
    # æ‰å¹³åŒ–æ‰€æœ‰æ•¸æ“š
    points = np.stack([x_coords.flatten(), y_coords.flatten(), z_coords.flatten()], axis=1)
    values = downsampled_logits.flatten()
    
    # æ ¹æ“šé–¾å€¼éæ¿¾é»
    mask = values > threshold
    filtered_points = points[mask]
    filtered_values = values[mask]
    
    print(f"Filtered points: {len(filtered_points)} / {len(points)}")
    
    # å°‡æ•¸å€¼æ˜ å°„åˆ°é¡è‰²ï¼ˆä½¿ç”¨matplotlib heatmapé¡è‰²æ˜ å°„ï¼‰
    # æ­£è¦åŒ–åˆ°0-1ç¯„åœ
    min_val, max_val = filtered_values.min(), filtered_values.max()
    normalized_values = (filtered_values - min_val) / (max_val - min_val) if max_val > min_val else np.zeros_like(filtered_values)
    
    # ä½¿ç”¨matplotlib viridisè‰²å½©æ˜ å°„ (æ·±è—->ç¶ ->é»ƒ->ç´…)
    # ä¹Ÿå¯ä»¥é¸æ“‡å…¶ä»–è‰²å½©æ˜ å°„: 'plasma', 'inferno', 'magma', 'cividis', 'hot', 'coolwarm'
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    
    # é¸æ“‡è‰²å½©æ˜ å°„ - æä¾›å¤šç¨®heatmapé¡è‰²é¸æ“‡
    colormap_obj = cm.get_cmap(colormap)
    
    # å°‡æ­£è¦åŒ–å€¼æ˜ å°„åˆ°RGBé¡è‰²
    colors_rgba = colormap_obj(normalized_values)
    colors = (colors_rgba[:, :3] * 255).astype(np.uint8)  # è½‰æ›ç‚º0-255ç¯„åœçš„RGB
    
    if format_type.lower() == "ply":
        output_file = f"{output_path}.ply"
        save_ply_pointcloud(filtered_points, colors, filtered_values, output_file)
    elif format_type.lower() == "xyz":
        output_file = f"{output_path}.xyz"
        save_xyz_pointcloud(filtered_points, colors, filtered_values, output_file)
    elif format_type.lower() == "pcd":
        output_file = f"{output_path}.pcd"
        save_pcd_pointcloud(filtered_points, colors, filtered_values, output_file)
    else:
        raise ValueError(f"Unsupported format: {format_type}")
    
    print(f"Point cloud saved to: {output_file}")
    return output_file

def save_ply_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºPLYæ ¼å¼ï¼ˆæœ€æ¨è–¦ï¼Œæ”¯æ´å¤šæ•¸3Dè»Ÿé«”ï¼‰"""
    with open(output_path, 'w') as f:
        # PLY header
        f.write("ply\n")
        f.write("format ascii 1.0\n")
        f.write(f"element vertex {len(points)}\n")
        f.write("property float x\n")
        f.write("property float y\n")
        f.write("property float z\n")
        f.write("property uchar red\n")
        f.write("property uchar green\n")
        f.write("property uchar blue\n")
        f.write("property float value\n")
        f.write("end_header\n")
        
        # é»æ•¸æ“š
        for i in range(len(points)):
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} "
                   f"{colors[i,0]} {colors[i,1]} {colors[i,2]} {values[i]:.6f}\n")

def save_xyz_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºXYZæ ¼å¼ï¼ˆç°¡å–®æ–‡å­—æ ¼å¼ï¼‰"""
    with open(output_path, 'w') as f:
        for i in range(len(points)):
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} "
                   f"{colors[i,0]} {colors[i,1]} {colors[i,2]} {values[i]:.6f}\n")

def save_pcd_pointcloud(points, colors, values, output_path):
    """å„²å­˜ç‚ºPCDæ ¼å¼ï¼ˆPCL libraryæ ¼å¼ï¼‰"""
    with open(output_path, 'w') as f:
        # PCD header
        f.write("# .PCD v0.7 - Point Cloud Data file format\n")
        f.write("VERSION 0.7\n")
        f.write("FIELDS x y z rgb value\n")
        f.write("SIZE 4 4 4 4 4\n")
        f.write("TYPE F F F U F\n")
        f.write("COUNT 1 1 1 1 1\n")
        f.write(f"WIDTH {len(points)}\n")
        f.write("HEIGHT 1\n")
        f.write("VIEWPOINT 0 0 0 1 0 0 0\n")
        f.write(f"POINTS {len(points)}\n")
        f.write("DATA ascii\n")
        
        # é»æ•¸æ“š
        for i in range(len(points)):
            # å°‡RGBæ‰“åŒ…æˆå–®ä¸€æ•´æ•¸
            rgb = (int(colors[i,0]) << 16) | (int(colors[i,1]) << 8) | int(colors[i,2])
            f.write(f"{points[i,0]:.3f} {points[i,1]:.3f} {points[i,2]:.3f} {rgb} {values[i]:.6f}\n")